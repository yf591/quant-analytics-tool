# ðŸš€ Quant Analytics Tool

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28%2B-red)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-In%20Development-yellow)]()

**A comprehensive prototype tool for quantitative financial data analysis and algorithmic trading**

This tool implements methodologies from "Advances in Financial Machine Learning" to provide integrated financial data analysis, strategy development, and backtesting capabilities using machine learning.

## ðŸŽ¯ Key Features

### ðŸ“Š Data Acquisition & Analysis
- **Multi-Source Data Integration**: Yahoo Finance, Alpha Vantage, Polygon APIs
- **Real-time Data Processing**: Live market data with <15 minute latency
- **Historical Data Management**: Comprehensive historical data storage and retrieval
- **Data Quality Assurance**: Automated validation, cleaning, and outlier detection
- **Multi-Asset Support**: Stocks, ETFs, FX, Cryptocurrencies, Options

### ðŸ§  Advanced Machine Learning
- **Deep Learning Models**: LSTM, Bidirectional LSTM, GRU architectures (âœ… Implemented)
- **Traditional ML**: Random Forest, XGBoost, Support Vector Machines
- **Ensemble Methods**: Model combination and meta-learning techniques
- **Feature Engineering**: 50+ technical indicators and advanced financial features
- **Auto-ML Pipeline**: Automated model selection and hyperparameter optimization

### ðŸ“ˆ Sophisticated Feature Engineering
- **Technical Indicators**: Complete TA-Lib integration with custom indicators
- **Advanced Features**: Fractal dimension, Hurst exponent, information-driven bars
- **Meta-Labeling**: Triple barrier method from "Advances in Financial ML"
- **Feature Selection**: Automated feature importance and selection algorithms
- **Real-time Computation**: Efficient streaming feature calculation

### ðŸ”™ Comprehensive Backtesting
- **Strategy Framework**: Extensible strategy development framework
- **Performance Analytics**: 20+ performance and risk metrics
- **Walk-Forward Analysis**: Time-series cross-validation for robust testing
- **Monte Carlo Simulation**: Statistical validation of strategy performance
- **Transaction Costs**: Realistic slippage and commission modeling

### âš–ï¸ Advanced Risk Management
- **Position Sizing**: Kelly criterion, risk parity, volatility targeting
- **Risk Metrics**: VaR, CVaR, maximum drawdown, Sharpe ratio, Sortino ratio
- **Portfolio Optimization**: Modern portfolio theory implementation
- **Real-time Monitoring**: Live risk monitoring and alerting system
- **Stress Testing**: Scenario analysis and sensitivity testing

### ðŸ“± Interactive Dashboard
- **Real-time Visualization**: Live updating charts and metrics
- **Customizable Interface**: Drag-and-drop dashboard configuration
- **Export Capabilities**: PDF reports, CSV data export, chart saving
- **Mobile Responsive**: Optimized for desktop, tablet, and mobile devices
- **Multi-language Support**: English interface with Japanese market support

## ðŸ›  Technology Stack

### Frontend
- **Streamlit**: Main dashboard
- **Plotly**: Interactive visualization
- **Altair**: Statistical visualization

### Backend & Data Processing
- **Python 3.9+**: Main development language
- **Pandas / NumPy**: Data manipulation & numerical computation
- **Scikit-learn**: Machine learning
- **TensorFlow/Keras**: Deep learning
- **TA-Lib**: Technical analysis
- **SQLite**: Local database

### Data Sources
- **yfinance**: Yahoo Finance API
- **pandas-datareader**: Various financial data sources
- **ccxt**: Cryptocurrency exchange APIs

### Backtesting
- **Backtrader**: Backtesting engine
- **Zipline**: Algorithmic trading framework

## ðŸš€ Quick Start

### Prerequisites
- **Python 3.9+** (recommended: Python 3.11)
- **Git** for version control
- **8GB RAM** minimum (16GB+ recommended)
- **10GB free disk space** for data and models

### 1. Environment Setup

```bash
# Clone repository
git clone https://github.com/yf591/quant-analytics-tool.git
cd quant-analytics-tool

# Create Python virtual environment
python -m venv venv

# Activate virtual environment
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# Upgrade pip and install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Install additional dependencies for development
pip install -r requirements-dev.txt  # Optional: for development tools
```

### 2. Configuration Setup

```bash
# Copy environment template
cp .env.example .env

# Edit configuration file
nano .env  # or use your preferred editor
```

#### Environment Variables Configuration
```bash
# Application Settings
DEBUG=false
LOG_LEVEL=INFO

# Data Source API Keys (optional for basic functionality)
ALPHA_VANTAGE_API_KEY=your_api_key_here
QUANDL_API_KEY=your_api_key_here
POLYGON_API_KEY=your_api_key_here

# Database Configuration
DATABASE_URL=sqlite:///./data/quant_analytics.db

# Cache Settings
CACHE_TTL=3600
REDIS_URL=redis://localhost:6379/0  # Optional: for advanced caching

# Security
SECRET_KEY=generate-a-strong-secret-key-here
```

### 3. Initial Data Setup

```bash
# Create necessary directories
mkdir -p data/raw data/processed models logs

# Initialize database
python scripts/init_database.py

# Download sample data (optional)
python scripts/download_sample_data.py
```

### 4. Launch Application

```bash
# Start Streamlit dashboard
streamlit run streamlit_app/main.py

# Alternative: Start with specific configuration
streamlit run streamlit_app/main.py --server.port=8501
```

Access the application at `http://localhost:8501`

### 5. Quick Tutorial

#### Basic Data Analysis Workflow:
1. **Navigate to Data Acquisition**: Select your preferred data source
2. **Choose Securities**: Enter stock symbols (e.g., AAPL, GOOGL, TSLA)
3. **Set Date Range**: Choose analysis period
4. **Generate Features**: Enable technical indicators and advanced features
5. **Train Models**: Select ML model type and train
6. **Run Backtest**: Evaluate strategy performance
7. **Analyze Results**: Review metrics and visualizations

#### Example: Quick S&P 500 Analysis
```python
# In the Streamlit interface:
# 1. Data Source: Yahoo Finance
# 2. Symbol: SPY
# 3. Date Range: 2020-01-01 to 2023-12-31
# 4. Features: Enable SMA, RSI, MACD
# 5. Model: LSTM
# 6. Strategy: Mean Reversion
```

## ðŸ“ Project Structure

```
quant-analytics-tool/
â”œâ”€â”€ ðŸ“ src/                         # Main source code
â”‚   â”œâ”€â”€ ðŸ“ data/                   # Data acquisition & processing
â”‚   â”‚   â”œâ”€â”€ collectors.py          # Data source collectors
â”‚   â”‚   â”œâ”€â”€ processors.py          # Data cleaning and preprocessing
â”‚   â”‚   â”œâ”€â”€ validators.py          # Data quality validation
â”‚   â”‚   â””â”€â”€ storage.py             # Data storage management
â”‚   â”œâ”€â”€ ðŸ“ features/               # Feature engineering
â”‚   â”‚   â”œâ”€â”€ technical.py           # Technical indicators
â”‚   â”‚   â”œâ”€â”€ advanced.py            # Advanced financial features
â”‚   â”‚   â”œâ”€â”€ labeling.py            # Meta-labeling methods
â”‚   â”‚   â””â”€â”€ pipeline.py            # Feature generation pipeline
â”‚   â”œâ”€â”€ ðŸ“ models/                 # Machine learning models
â”‚   â”‚   â”œâ”€â”€ base.py                # Base model classes
â”‚   â”‚   â”œâ”€â”€ evaluation.py          # Model evaluation
â”‚   â”‚   â”œâ”€â”€ traditional/           # Traditional ML models
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ random_forest.py   # Random Forest
â”‚   â”‚   â”‚   â”œâ”€â”€ svm_model.py       # Support Vector Machine
â”‚   â”‚   â”‚   â””â”€â”€ xgboost_model.py   # XGBoost
â”‚   â”‚   â”œâ”€â”€ deep_learning/         # Deep learning models (âœ… Implemented)
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py        # Package initialization
â”‚   â”‚   â”‚   â”œâ”€â”€ lstm.py            # LSTM implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ gru.py             # GRU implementations
â”‚   â”‚   â”‚   â””â”€â”€ utils.py           # Deep learning utilities
â”‚   â”‚   â””â”€â”€ ensemble.py            # Ensemble methods (planned)
â”‚   â”œâ”€â”€ ðŸ“ backtesting/            # Backtesting framework
â”‚   â”‚   â”œâ”€â”€ engine.py              # Backtesting engine
â”‚   â”‚   â”œâ”€â”€ strategies.py          # Trading strategies
â”‚   â”‚   â”œâ”€â”€ metrics.py             # Performance metrics
â”‚   â”‚   â””â”€â”€ portfolio.py           # Portfolio management
â”‚   â”œâ”€â”€ ðŸ“ risk/                   # Risk management
â”‚   â”‚   â”œâ”€â”€ position_sizing.py     # Position sizing algorithms
â”‚   â”‚   â”œâ”€â”€ risk_metrics.py        # Risk calculations
â”‚   â”‚   â”œâ”€â”€ portfolio_opt.py       # Portfolio optimization
â”‚   â”‚   â””â”€â”€ stress_testing.py      # Stress testing
â”‚   â”œâ”€â”€ ðŸ“ visualization/          # Visualization components
â”‚   â”‚   â”œâ”€â”€ charts.py              # Chart generation
â”‚   â”‚   â”œâ”€â”€ dashboards.py          # Dashboard components
â”‚   â”‚   â”œâ”€â”€ reports.py             # Report generation
â”‚   â”‚   â””â”€â”€ utils.py               # Visualization utilities
â”‚   â”œâ”€â”€ ðŸ“ utils/                  # Utility functions
â”‚   â”‚   â”œâ”€â”€ logging.py             # Logging configuration
â”‚   â”‚   â”œâ”€â”€ helpers.py             # Helper functions
â”‚   â”‚   â””â”€â”€ decorators.py          # Custom decorators
â”‚   â””â”€â”€ config.py                  # Configuration management
â”œâ”€â”€ ðŸ“ streamlit_app/              # Streamlit application
â”‚   â”œâ”€â”€ ðŸ“ pages/                  # Streamlit pages
â”‚   â”‚   â”œâ”€â”€ 01_data_acquisition.py # Data acquisition page
â”‚   â”‚   â”œâ”€â”€ 02_feature_engineering.py # Feature engineering page
â”‚   â”‚   â”œâ”€â”€ 03_model_training.py   # Model training page
â”‚   â”‚   â”œâ”€â”€ 04_backtesting.py      # Backtesting page
â”‚   â”‚   â””â”€â”€ 05_analysis.py         # Results analysis page
â”‚   â”œâ”€â”€ ðŸ“ components/             # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ sidebar.py             # Sidebar components
â”‚   â”‚   â”œâ”€â”€ charts.py              # Chart components
â”‚   â”‚   â””â”€â”€ forms.py               # Form components
â”‚   â”œâ”€â”€ ðŸ“ utils/                  # Streamlit utilities
â”‚   â”‚   â”œâ”€â”€ session_state.py       # Session state management
â”‚   â”‚   â””â”€â”€ helpers.py             # UI helper functions
â”‚   â””â”€â”€ main.py                    # Main application entry
â”œâ”€â”€ ðŸ“ tests/                      # Test suite
â”‚   â”œâ”€â”€ conftest.py                # Pytest configuration
â”‚   â”œâ”€â”€ test_collectors.py         # Data collector tests
â”‚   â”œâ”€â”€ test_feature_pipeline.py   # Feature pipeline tests
â”‚   â”œâ”€â”€ ðŸ“ test_analysis/          # Analysis module tests
â”‚   â”œâ”€â”€ ðŸ“ features/               # Feature engineering tests
â”‚   â””â”€â”€ ðŸ“ models/                 # ML model tests
â”‚       â””â”€â”€ test_traditional_models.py  # Traditional ML tests
â”œâ”€â”€ ðŸ“ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ init_database.py           # Database initialization
â”‚   â”œâ”€â”€ download_sample_data.py    # Sample data download
â”‚   â”œâ”€â”€ train_models.py            # Batch model training
â”‚   â””â”€â”€ generate_report.py         # Report generation
â”œâ”€â”€ ðŸ“ docs/                       # Documentation
â”‚   â”œâ”€â”€ api/                       # API documentation
â”‚   â”œâ”€â”€ tutorials/                 # User tutorials
â”‚   â””â”€â”€ developer_guide.md         # Developer guide
â”œâ”€â”€ ðŸ“ data/                       # Local data storage
â”‚   â”œâ”€â”€ ðŸ“ raw/                    # Raw data files
â”‚   â”œâ”€â”€ ðŸ“ processed/              # Processed data
â”‚   â””â”€â”€ ðŸ“ external/               # External datasets
â”œâ”€â”€ ðŸ“ models/                     # Saved ML models
â”‚   â”œâ”€â”€ ðŸ“ trained/                # Trained models
â”‚   â”œâ”€â”€ ðŸ“ checkpoints/            # Training checkpoints
â”‚   â””â”€â”€ ðŸ“ configs/                # Model configurations
â”œâ”€â”€ ðŸ“ logs/                       # Application logs
â”œâ”€â”€ ðŸ“ configs/                    # Configuration files
â”œâ”€â”€ ðŸ“„ requirements.txt            # Production dependencies
â”œâ”€â”€ ðŸ“„ requirements-dev.txt        # Development dependencies
â”œâ”€â”€ ðŸ“„ .env.example                # Environment template
â”œâ”€â”€ ðŸ“„ .gitignore                  # Git ignore rules
â”œâ”€â”€ ðŸ“„ README.md                   # Project documentation
â”œâ”€â”€ ðŸ“„ SPECIFICATION.md            # Technical specifications
â”œâ”€â”€ ðŸ“„ Architecture_&_Visual_Reference.md # Architecture documentation
â””â”€â”€ ðŸ“„ LICENSE                     # MIT License
```

### Key Directories Explained

#### `/src/` - Core Application Logic
- **data/**: Data acquisition, processing, and storage
- **features/**: Feature engineering and technical analysis
- **models/**: Machine learning model implementations
- **backtesting/**: Strategy testing and performance evaluation
- **risk/**: Risk management and portfolio optimization
- **visualization/**: Charts, dashboards, and reporting

#### `/streamlit_app/` - User Interface
- **pages/**: Individual dashboard pages
- **components/**: Reusable UI components
- **utils/**: UI-specific utilities and helpers

#### `/tests/` - Quality Assurance
- **unit/**: Individual component testing
- **integration/**: End-to-end workflow testing
- **fixtures/**: Test data and mock objects

#### `/scripts/` - Automation & Utilities
- Database setup and maintenance
- Data download and preprocessing
- Batch model training
- Report generation

## ðŸ’¡ Planned Features

### ðŸ“Š Phase 1: Foundation (Weeks 1-3) âœ… **COMPLETED**
- [x] **Project Setup**: Complete infrastructure and configuration
- [x] **Data Acquisition**: Yahoo Finance integration with yFinance
- [x] **Data Validation**: Comprehensive data quality validation system
- [x] **Data Storage**: Local SQLite database with optimized performance
- [x] **Basic Analysis**: AFML-compliant return, volatility, statistical, and correlation analysis
- [x] **Logging System**: Comprehensive logging with rotation and level management
- [x] **Testing Framework**: Unit testing with pytest and comprehensive test coverage

**Key Achievements**:
- âœ… Complete data acquisition pipeline (YFinanceCollector)
- âœ… Advanced data validation (DataValidator with 3-tier validation)
- âœ… Optimized SQLite storage (SQLiteStorage with WAL mode)
- âœ… Four core analysis modules (returns, volatility, statistics, correlation)
- âœ… Risk metrics (VaR, CVaR, Sharpe, Sortino, Calmar ratios)
- âœ… Distribution analysis (normality, autocorrelation, ARCH tests)
- âœ… Comprehensive error handling and performance optimization

### ðŸ§  Phase 2: Feature Engineering (Weeks 4-6) âœ… **COMPLETED**
- [x] **Technical Indicators**: Core indicators (SMA, EMA, RSI, MACD, Bollinger Bands) âœ… **COMPLETED**
- [x] **Advanced Features**: Fractal dimension, Hurst exponent, information-driven bars âœ… **COMPLETED**
- [x] **Meta-Labeling**: Triple barrier method from "Advances in Financial ML" âœ… **COMPLETED**
- [x] **Fractional Differentiation**: Stationarity with memory preservation âœ… **COMPLETED**
- [x] **Feature Pipeline**: Automated feature generation and selection âœ… **COMPLETED**
- [x] **Feature Validation**: Quality checks and importance analysis âœ… **COMPLETED**

**Week 4 Achievements**:
- âœ… 10+ professional technical indicators implemented
- âœ… Comprehensive test suite (26 test cases, 100% pass rate)
- âœ… AFML-compliant calculations with error handling
- âœ… Real-time analysis capabilities and signal generation

**Week 5 Achievements**:
- âœ… Fractal dimension analysis (Higuchi & Box-counting methods)
- âœ… Hurst exponent calculation (R/S Analysis & DFA) for market regime identification
- âœ… Information-driven bars (Tick, Volume, Dollar bars) for superior data sampling
- âœ… Triple barrier method for sophisticated meta-labeling in ML
- âœ… Fractional differentiation for achieving stationarity while preserving memory
- âœ… 21 comprehensive test cases with full coverage of advanced features

### ðŸ¤– Phase 3: Machine Learning Models (Weeks 7-10) **IN PROGRESS**
- [x] **Traditional ML**: Random Forest, XGBoost, Support Vector Machines âœ… **Week 7 COMPLETED**
- [x] **Deep Learning**: LSTM, Bidirectional LSTM, GRU architectures âœ… **Week 8 COMPLETED**
- [ ] **Advanced Models**: Transformer architecture for time series prediction
- [ ] **Ensemble Methods**: Model combination and meta-learning techniques
- [ ] **AutoML Pipeline**: Automated model selection and hyperparameter optimization
- [x] **Model Evaluation**: Comprehensive comparison and validation framework âœ… **Week 7 COMPLETED**

**Week 7 Achievements**:
- âœ… Random Forest Classifier & Regressor with quantile predictions and feature importance
- âœ… Support Vector Machine implementation with kernel methods and automatic scaling
- âœ… XGBoost integration with gradient boosting optimization
- âœ… Comprehensive base model framework with abstract classes and factory pattern
- âœ… Advanced evaluation system with financial metrics (Sharpe, Sortino, max drawdown)
- âœ… AFML-compliant implementations following Chapter 6 ensemble methods
- âœ… Cross-validation framework with time-series aware splitting
- âœ… Model persistence system with joblib serialization
- âœ… Professional test suite with 99% implementation completion and 100% test success rate

**Week 8 Achievements**:
- âœ… LSTM Classifier & Regressor with sequence modeling and financial time series optimization
- âœ… Bidirectional LSTM implementation for enhanced temporal pattern recognition
- âœ… GRU architecture with efficient computation and memory management
- âœ… Monte Carlo Dropout for uncertainty estimation in neural network predictions
- âœ… Advanced hyperparameter tuning system with Bayesian optimization
- âœ… Comprehensive model comparison framework with financial metrics integration
- âœ… Deep learning utilities for preprocessing and feature scaling
- âœ… Complete test suite with 100% coverage and performance validation

### ðŸ“ˆ Phase 4: Backtesting & Risk Management (Weeks 11-13)
- [ ] **Backtesting Engine**: Core framework for strategy testing
- [ ] **Walk-Forward Analysis**: Time-series cross-validation for robust testing
- [ ] **Monte Carlo Simulation**: Statistical validation of strategy performance
- [ ] **Risk Management**: Kelly criterion, VaR/CVaR, portfolio optimization
- [ ] **Performance Analytics**: 20+ performance and risk metrics
- [ ] **Transaction Costs**: Realistic slippage and commission modeling

### ðŸ”§ Phase 5: Integration & Optimization (Weeks 14-15)
- [ ] **End-to-End Integration**: Complete workflow from data to predictions
- [ ] **Performance Optimization**: System speed and memory efficiency
- [ ] **API Development**: RESTful API endpoints for external access
- [ ] **Testing & Documentation**: Comprehensive testing and user guides
- [ ] **Deployment Preparation**: Production-ready system configuration

### ï¿½ Future Expansion (Post Phase 5)
- [ ] Real-time trading API integration
- [ ] Cloud deployment (AWS/GCP/Azure)
- [ ] Multi-asset portfolio backtesting
- [ ] Advanced charting and visualization
- [ ] Mobile-responsive design
- [ ] Multi-language support
- [ ] Institutional-grade features

## ðŸ›  Development Guidelines

### Setting Up Development Environment

```bash
# Clone the repository
git clone https://github.com/yf591/quant-analytics-tool.git
cd quant-analytics-tool

# Create development environment
python -m venv venv-dev
source venv-dev/bin/activate

# Install development dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run initial tests
pytest tests/
```

### Code Style & Standards

- **Python Style**: Follow PEP 8 with Black formatter
- **Type Hints**: Required for all public functions
- **Docstrings**: Google-style docstrings
- **Testing**: Minimum 80% test coverage
- **Linting**: Use flake8, mypy, and pylint

### Development Workflow

1. **Create Feature Branch**: `git checkout -b feature/your-feature-name`
2. **Implement Changes**: Follow TDD approach
3. **Run Tests**: `pytest tests/`
4. **Code Review**: Submit pull request
5. **Integration**: Merge after approval

### Testing Strategy

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src tests/

# Run specific test categories
pytest tests/unit/          # Unit tests
pytest tests/integration/   # Integration tests
pytest tests/performance/   # Performance tests

# Run tests with markers
pytest -m "not slow"        # Skip slow tests
pytest -m "data"            # Run only data-related tests
```

### Performance Profiling

```bash
# Profile application performance
python -m cProfile -o profile.stats scripts/profile_app.py

# Memory profiling
python -m memory_profiler scripts/memory_test.py

# Line profiling
kernprof -l -v scripts/line_profile.py
```

## ðŸ› Troubleshooting

### Common Issues & Solutions

#### Installation Issues

**Problem**: `pip install` fails with dependency conflicts
```bash
# Solution: Use conda environment
conda create -n quant-env python=3.11
conda activate quant-env
pip install -r requirements.txt
```

**Problem**: TA-Lib installation fails
```bash
# macOS with Homebrew
brew install ta-lib
pip install TA-Lib

# Ubuntu/Debian
sudo apt-get install libta-lib-dev
pip install TA-Lib

# Windows
# Download wheel from: https://www.lfd.uci.edu/~gohlke/pythonlibs/#ta-lib
pip install TA_Lib-0.4.24-cp311-cp311-win_amd64.whl
```

#### Runtime Issues

**Problem**: "ModuleNotFoundError" when running Streamlit
```bash
# Solution: Add src to Python path
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"
streamlit run streamlit_app/main.py
```

**Problem**: Memory issues with large datasets
```bash
# Solution: Adjust chunk size in config
# Edit src/config.py:
DATA_CHUNK_SIZE = 10000  # Reduce from default
```

**Problem**: API rate limit errors
```bash
# Solution: Implement exponential backoff
# Check data/collectors.py for rate limiting settings
RATE_LIMIT_DELAY = 1.0  # Increase delay between requests
```

#### Performance Issues

**Problem**: Slow dashboard loading
- Enable caching: Set `CACHE_TTL=3600` in .env
- Reduce data range: Limit historical data to 2 years
- Use data sampling: Enable in advanced settings

**Problem**: High memory usage
- Monitor with: `htop` or Activity Monitor
- Optimize with: `memory_profiler`
- Reduce batch size in ML training

### Debugging Tools

#### Logging Configuration
```python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Log to file
import logging
logging.basicConfig(
    filename='debug.log',
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

#### Development Server
```bash
# Run with debug mode
export DEBUG=true
streamlit run streamlit_app/main.py

# Run with hot reload
streamlit run streamlit_app/main.py --server.runOnSave=true
```

### Getting Help

- **Documentation**: Check `/docs` directory
- **Issues**: Create GitHub issue with detailed description
- **Discussions**: Use GitHub Discussions for questions
- **Email**: Contact maintainers for urgent issues

## ðŸ”§ API Documentation

### REST API Endpoints (Future)

```bash
# Get available symbols
GET /api/v1/symbols?exchange=NYSE

# Fetch price data
GET /api/v1/data/{symbol}?start=2023-01-01&end=2023-12-31

# Generate features
POST /api/v1/features
{
  "symbol": "AAPL",
  "indicators": ["sma", "rsi", "macd"],
  "timeframe": "1d"
}

# Train model
POST /api/v1/models/train
{
  "model_type": "lstm",
  "features": [...],
  "parameters": {...}
}

# Get predictions
GET /api/v1/predictions/{model_id}/{symbol}

# Run backtest
POST /api/v1/backtest
{
  "strategy": "mean_reversion",
  "symbols": ["AAPL", "GOOGL"],
  "start_date": "2023-01-01",
  "end_date": "2023-12-31"
}
```

### Python API Usage

```python
from src.data.collectors import YFinanceCollector
from src.features.technical import TechnicalIndicators
from src.models.lstm import LSTMModel

# Data acquisition
collector = YFinanceCollector()
data = collector.fetch_data("AAPL", "2023-01-01", "2023-12-31")

# Feature engineering
indicators = TechnicalIndicators()
features = indicators.generate_all(data)

# Model training
model = LSTMModel()
model.train(features, targets)
predictions = model.predict(test_features)
```

## ðŸ“š References

This project is developed with the following books as primary references:

- **Marcos LÃ³pez de Prado**: "Advances in Financial Machine Learning"
- **Stefan Jansen**: "Machine Learning for Algorithmic Trading"
- **Yves Hilpisch**: "Python for Finance"

## ðŸ¤ Contributing

Contributions to the project are welcome!

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ðŸ“„ License

This project is released under the MIT License. See the [LICENSE](LICENSE) file for details.

## âš ï¸ Disclaimer

This tool is developed for educational and research purposes and does not constitute investment advice. Use for actual investment decisions is at your own risk. Past performance does not guarantee future results.

## ðŸ“ž Support

- ðŸ› Bug Reports: [Issues](https://github.com/yf591/quant-analytics-tool/issues)
- ðŸ’¡ Feature Requests: [Discussions](https://github.com/yf591/quant-analytics-tool/discussions)

---

â­ If you find this project helpful, please consider giving it a star!
